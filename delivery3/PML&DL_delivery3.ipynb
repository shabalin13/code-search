{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabalin13/code-search/blob/main/delivery3/PML%26DL_delivery3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_9XKAZsOrkx"
      },
      "source": [
        "#Delivery 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwgJ6sOwJpVi"
      },
      "source": [
        "##Implementing search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkGRYCMkuzWR",
        "outputId": "8fdd530c-aee8-4d1d-e610-7754162f1487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "EMBEDDINGS_PRECOMPUTED = True\n",
        "EMBEDDINGS_ON_GOOGLE_DRIVE = True\n",
        "if EMBEDDINGS_ON_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZVy_I93rRkw",
        "outputId": "44a3807c-b5bf-48be-9753-516994518c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 4%\r\rReading package lists... 4%\r\rReading package lists... 4%\r\rReading package lists... 4%\r"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install datasets --quiet\n",
        "!apt install libomp-dev\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "from enum import Enum, auto\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    !pip install faiss-gpu -q\n",
        "else:\n",
        "    !pip install faiss -q\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", 'python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kAFcx2gPNuf"
      },
      "outputs": [],
      "source": [
        "train, valid, test = dataset['train'], dataset['validation'], dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsWA3IKdSnQd"
      },
      "outputs": [],
      "source": [
        "class SeqType(Enum):\n",
        "  CODE = auto()\n",
        "  DOC = auto()\n",
        "\n",
        "\n",
        "class TokenizeCollator(object):\n",
        "    def __init__(self, tokenizer, seq_type):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_type = seq_type\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.create_one_batch(batch)\n",
        "\n",
        "    def create_one_batch(self, batch):\n",
        "        tokens_batch = list(map(lambda item: self.get_formatted_input(item), batch))\n",
        "        batch_encoding = self.tokenizer(tokens_batch, padding=True, return_tensors='pt', return_token_type_ids=True, truncation=True)\n",
        "        tokens_ids = batch_encoding.input_ids.to(device)\n",
        "        token_type_ids = batch_encoding.token_type_ids.to(device)\n",
        "        attention_mask = batch_encoding.attention_mask.to(device)\n",
        "        return tokens_ids, token_type_ids, attention_mask\n",
        "\n",
        "    def get_formatted_input(self, item):\n",
        "        if self.seq_type == SeqType.CODE:\n",
        "            return self.get_formatted_input_for_code(item)\n",
        "        elif self.seq_type == SeqType.DOC:\n",
        "            return self.get_formatted_input_for_doc(item)\n",
        "        else:\n",
        "            raise Exception(\"Incorrect sequence type\")\n",
        "\n",
        "    def get_formatted_input_for_code(self, item):\n",
        "        doc_tokens = ' '.join(item['docstring_tokens'])\n",
        "        code_tokens = ' '.join(item['code_tokens'])\n",
        "        formatted_input = self.tokenizer.cls_token + doc_tokens + self.tokenizer.sep_token+code_tokens + self.tokenizer.sep_token\n",
        "        return formatted_input\n",
        "\n",
        "    def get_formatted_input_for_doc(self, item):\n",
        "        doc_tokens = ' '.join(item['docstring_tokens'])\n",
        "        code_tokens = ''\n",
        "        formatted_input = self.tokenizer.cls_token + doc_tokens + self.tokenizer.sep_token+code_tokens + self.tokenizer.sep_token\n",
        "        return formatted_input \n",
        "\n",
        "\n",
        "code_tokenize_collate_fn = TokenizeCollator(tokenizer, SeqType.CODE)\n",
        "doc_tokenize_collate_fn = TokenizeCollator(tokenizer, SeqType.DOC)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "test_code_tokens_ids = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=code_tokenize_collate_fn, num_workers=0)\n",
        "test_doc_tokens_ids = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=doc_tokenize_collate_fn, num_workers=0)\n",
        "\n",
        "# for idx, batch in enumerate(test_tokens_ids):\n",
        "#   # print(batch.shape)\n",
        "#   print(batch)\n",
        "#   if idx >= 0:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0r54Nk7cJLK"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def print_gpu_memory_usage(idx=''):\n",
        "    print(idx)\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "    print()\n",
        "\n",
        "\n",
        "if not EMBEDDINGS_PRECOMPUTED:\n",
        "    batched_test_code_embs = []\n",
        "    for batch in tqdm(test_code_tokens_ids):\n",
        "        tokens_ids, token_type_ids, attention_mask = batch\n",
        "        with torch.no_grad():\n",
        "            embs = model(input_ids=tokens_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
        "            batched_test_code_embs.append(embs)\n",
        "\n",
        "    batched_test_doc_embs = []\n",
        "    for batch in tqdm(test_doc_tokens_ids):\n",
        "        tokens_ids, token_type_ids, attention_mask = batch\n",
        "        with torch.no_grad():\n",
        "            embs = model(input_ids=tokens_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
        "            batched_test_doc_embs.append(embs)\n",
        "\n",
        "    test_code_embeddings = torch.cat(batched_test_code_embs, dim=0)\n",
        "    test_doc_embeddings = torch.cat(batched_test_doc_embs, dim=0)\n",
        "    if EMBEDDINGS_ON_GOOGLE_DRIVE:\n",
        "        %cd /content/drive/MyDrive/PML&DL/Project\n",
        "    torch.save(test_code_embeddings, 'test_code_embeddings.pt')\n",
        "    torch.save(test_doc_embeddings, 'test_doc_embeddings.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cph_ZnGjEa7"
      },
      "outputs": [],
      "source": [
        "if EMBEDDINGS_PRECOMPUTED:\n",
        "    if EMBEDDINGS_ON_GOOGLE_DRIVE:\n",
        "        %cd /content/drive/MyDrive/PML&DL/Project\n",
        "    test_code_embeddings = torch.load('test_code_embeddings.pt', map_location=torch.device('cpu'))\n",
        "    test_doc_embeddings = torch.load('test_doc_embeddings.pt', map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAueILqgAJkh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "class FaissKNeighbors:\n",
        "    def __init__(self, is_cuda):\n",
        "        self.index = None\n",
        "        self.is_cuda = is_cuda\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
        "        if self.is_cuda:\n",
        "            res = faiss.StandardGpuResources()\n",
        "            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)\n",
        "        if type(X) == torch.Tensor:\n",
        "            X = X.numpy()\n",
        "        self.index.add(X)\n",
        "\n",
        "    def predict(self, X, k):\n",
        "        if type(X) == torch.Tensor:\n",
        "            X = X.numpy()\n",
        "        distances, indices = self.index.search(X, k=k)\n",
        "        return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE9YRj_dAKKP"
      },
      "outputs": [],
      "source": [
        "test_faiss = FaissKNeighbors(is_cuda=device=='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woQie9eA6pno"
      },
      "outputs": [],
      "source": [
        "k = 1000\n",
        "mrrs = []\n",
        "for beg_idx in tqdm(range(0, len(test_code_embeddings), k)):\n",
        "    if beg_idx + k > len(test_code_embeddings):\n",
        "        break\n",
        "    doc_embs_subset = test_doc_embeddings[beg_idx:beg_idx + k]\n",
        "    code_embs_subset = test_code_embeddings[beg_idx:beg_idx + k]\n",
        "    test_faiss.fit(code_embs_subset)  \n",
        "    preds = test_faiss.predict(doc_embs_subset, k=k)\n",
        "\n",
        "    targets = np.repeat(np.expand_dims(range(k), 1), k, axis=1)\n",
        "\n",
        "    reciprocal_ranks = 1 / (np.argwhere(np.equal(preds, targets))[:,1] + 1)\n",
        "    mrr_ = np.mean(reciprocal_ranks)\n",
        "    mrrs.append(mrr_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH4x4_AmvzKM"
      },
      "outputs": [],
      "source": [
        "mrr = np.mean(mrrs)\n",
        "print('Mean Reciprocal rank is: ', mrr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delivery 3"
      ],
      "metadata": {
        "id": "xrOnff1tIHLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FineTunedCodeBert:\n",
        "#     def __init__(self, model, is_freeze_bert=True):\n",
        "#         self.model = model\n",
        "\n",
        "#         if is_freeze_bert:\n",
        "#             for p in self.model.parameters():\n",
        "#                 p.requires_grad = False\n",
        "#             for p in self.model.pooler.parameters():\n",
        "#                 p.requires_grad = True\n",
        "\n",
        "#     def forward(self, X):\n",
        "#         tokens_ids, token_type_ids, attention_mask = X\n",
        "#         embs = self.model(input_ids=tokens_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
        "#         return embs\n",
        "\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.pooler.parameters():\n",
        "    p.requires_grad = True"
      ],
      "metadata": {
        "id": "NeW0rxvKIOzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "epochs = 8\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "3cXmwYZBSBlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(code_dataloader, doc_dataloader, epoch):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for iteration, (code_tokens, doc_tokens) in tqdm(enumerate(zip(code_dataloader, doc_dataloader))):\n",
        "        optimizer.zero_grad()\n",
        "        tokens_ids, token_type_ids, attention_mask = code_tokens\n",
        "        code_embs = model(input_ids=tokens_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
        "        tokens_ids, token_type_ids, attention_mask = doc_tokens\n",
        "        doc_embs = model(input_ids=tokens_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
        "        loss = loss_fn(doc_embs, code_embs)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if iteration % 50 == 0:\n",
        "            _loss = running_loss / (iteration + 1)\n",
        "            print(\"epoch: {}\\titeration: {}\\tloss: {}\\tthis iteration loss: {}\".format(epoch, iteration, _loss, loss))\n",
        "\n",
        "    torch.save(model, 'codebert.pt')"
      ],
      "metadata": {
        "id": "V0iAdf1UTSbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_code_model_input = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False, collate_fn=code_tokenize_collate_fn, num_workers=0)\n",
        "train_doc_model_input = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False, collate_fn=doc_tokenize_collate_fn, num_workers=0)\n",
        "val_code_model_input = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=code_tokenize_collate_fn, num_workers=0)\n",
        "val_doc_model_input = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=doc_tokenize_collate_fn, num_workers=0)\n"
      ],
      "metadata": {
        "id": "PoUXAoaOX9rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train(train_code_model_input, train_doc_model_input, epoch)"
      ],
      "metadata": {
        "id": "tinECtM3XURX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.pooler.parameters():\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "482gJv27Ms0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}